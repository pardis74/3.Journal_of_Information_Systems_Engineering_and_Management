import numpy as np
import pywt
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
import random

# For conceptual demonstration, we'll use dummy data for training and testing.
# In a real scenario, this data would come from PSCAD simulations and DWT feature extraction.

class DataLoader:
    """
    A class to simulate loading and preparing data for the LSTM and GA.
    In a real application, this would involve processing data from PSCAD simulations
    and performing DWT feature extraction as described in the paper.
    """
    def __init__(self, num_samples=1000, sequence_length=18, num_classes=2):
        self.num_samples = num_samples
        [cite_start]self.sequence_length = sequence_length # Corresponds to 18 time-series features (6 detail coefficients for 3 phases) [cite: 97]
        self.num_classes = num_classes
        self.X, self.y = self._generate_dummy_data()

    def _generate_dummy_data(self):
        """
        Generates dummy time-series data for demonstration purposes.
        In a real scenario, this would be actual DWT coefficients from voltage waveforms.
        """
        X = np.random.rand(self.num_samples, self.sequence_length, 1) # LSTM expects 3D input: (samples, timesteps, features)
        y = np.random.randint(0, self.num_classes, self.num_samples) # Binary classification: ferroresonance (1) or not (0)
        print(f"Generated dummy data: X shape {X.shape}, y shape {y.shape}")
        return X, y

    def get_data(self):
        """
        Returns the generated dataset.
        """
        return self.X, self.y

class WaveletTransform:
    """
    Encapsulates Discrete Wavelet Transform (DWT) functionalities for feature extraction.
    As per the paper, DWT is used to decompose voltage waveforms into detail and
    approximation coefficients.
    """
    [cite_start]def __init__(self, wavelet='db4', level=6): # Paper mentions up to six levels of coefficients [cite: 5, 94]
        self.wavelet = wavelet
        self.level = level

    def decompose(self, signal):
        """
        Performs multi-level DWT decomposition on a single signal.
        Returns approximation and detail coefficients for each level.
        [cite_start]The paper uses detail coefficients as input features for LSTM[cite: 153].
        """
        if not isinstance(signal, (np.ndarray, list)):
            raise ValueError("Input signal must be a numpy array or list.")
        
        # Perform DWT decomposition
        coefficients = pywt.wavedec(signal, self.wavelet, level=self.level)
        
        # The first element is the approximation coefficients, the rest are detail coefficients
        approximation_coeffs = coefficients[0]
        detail_coeffs = coefficients[1:] # A list of arrays, one for each level of detail coefficients

        print(f"Wavelet decomposition performed. Approximation coeffs shape: {approximation_coeffs.shape}")
        print(f"Detail coeffs (list of shapes): {[d.shape for d in detail_coeffs]}")
        
        return approximation_coeffs, detail_coeffs

class LSTMModel:
    """
    Represents the Long Short-Term Memory (LSTM) neural network for classification.
    """
    def __init__(self, input_shape, num_classes=2, num_hidden_units=50, learning_rate=0.001):
        self.input_shape = input_shape # (sequence_length, num_features_per_timestep)
        self.num_classes = num_classes
        [cite_start]self.num_hidden_units = num_hidden_units # Max 50 hidden layers mentioned [cite: 99]
        [cite_start]self.learning_rate = learning_rate # Set at 0.001 [cite: 100]
        self.model = self._build_model()

    def _build_model(self):
        """
        Builds the Keras LSTM model.
        """
        model = Sequential([
            LSTM(self.num_hidden_units, activation='tanh', input_shape=self.input_shape),
            Dropout(0.2), # Adding dropout for regularization
            Dense(self.num_classes, activation='softmax') if self.num_classes > 1 else Dense(1, activation='sigmoid')
        ])
        optimizer = Adam(learning_rate=self.learning_rate)
        model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy' if self.num_classes > 1 else 'binary_crossentropy', metrics=['accuracy'])
        print(f"LSTM model built with {self.num_hidden_units} hidden units and learning rate {self.learning_rate}.")
        model.summary()
        return model

    def train(self, X_train, y_train, epochs=15, batch_size=32, validation_split=0.2):
        """
        Trains the LSTM model.
        [cite_start]MaxEpochs is considered 15[cite: 100].
        """
        print(f"Training LSTM model for {epochs} epochs...")
        history = self.model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, verbose=0)
        print("LSTM model training complete.")
        return history

    def evaluate(self, X_test, y_test):
        """
        Evaluates the LSTM model's performance on test data.
        """
        print("Evaluating LSTM model...")
        loss, accuracy = self.model.evaluate(X_test, y_test, verbose=0)
        y_pred_probs = self.model.predict(X_test, verbose=0)
        y_pred = np.argmax(y_pred_probs, axis=1) if self.num_classes > 1 else (y_pred_probs > 0.5).astype(int)

        precision = precision_score(y_test, y_pred, average='binary' if self.num_classes == 2 else 'weighted')
        recall = recall_score(y_test, y_pred, average='binary' if self.num_classes == 2 else 'weighted')
        f1 = f1_score(y_test, y_pred, average='binary' if self.num_classes == 2 else 'weighted')

        metrics = {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1
        }
        print("LSTM model evaluation complete.")
        return metrics

class GeneticAlgorithm:
    """
    Implements a Genetic Algorithm to optimize LSTM parameters.
    Optimizes `num_hidden_units` and `epochs` based on a fitness function.
    """
    [cite_start]def __init__(self, data_loader, pop_size=100, num_generations=15, # Num iterations [cite: 154]
                 [cite_start]mutation_rate=0.3, crossover_rate=0.7): # Probabilities [cite: 154]
        self.data_loader = data_loader
        self.pop_size = pop_size
        self.num_generations = num_generations
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate
        self.X, self.y = self.data_loader.get_data()
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42
        )
        print(f"GA initialized with pop_size={pop_size}, num_generations={num_generations}")

    def _initialize_population(self):
        """
        Initializes the first population of chromosomes.
        Each chromosome contains (num_hidden_units, max_epochs, W_acc, W_time).
        [cite_start]Paper mentions num_hidden_units up to 50, max_epochs 15[cite: 99, 100].
        W_acc and W_time are weight coefficients for accuracy and time in fitness function.
        """
        population = []
        for _ in range(self.pop_size):
            [cite_start]num_hidden_units = random.randint(10, 50) # Example range, max 50 [cite: 99]
            [cite_start]max_epochs = random.randint(5, 15) # Example range, max 15 [cite: 100]
            # Random weights for accuracy and time in the fitness function
            W_acc = random.uniform(0.5, 1.5)
            W_time = random.uniform(0.5, 1.5)
            population.append({'num_hidden_units': num_hidden_units, 'max_epochs': max_epochs, 'W_acc': W_acc, 'W_time': W_time})
        print(f"Initial population of {self.pop_size} chromosomes created.")
        return population

    def _calculate_fitness(self, chromosome):
        """
        Calculates the fitness of a chromosome by training and evaluating an LSTM model.
        [cite_start]Fitness function: fitness_GA = (W_acc * LSTM_Accuracy) / (1 + W_time * LSTM_Training_Time) [cite: 103]
        """
        num_hidden_units = chromosome['num_hidden_units']
        max_epochs = chromosome['max_epochs']
        W_acc = chromosome['W_acc']
        W_time = chromosome['W_time']

        # Train and evaluate LSTM model
        lstm_model = LSTMModel(input_shape=(self.data_loader.sequence_length, 1),
                               num_hidden_units=num_hidden_units,
                               [cite_start]learning_rate=0.001) # Learning rate is fixed as per paper [cite: 100]
        
        import time
        start_time = time.time()
        history = lstm_model.train(self.X_train, self.y_train, epochs=max_epochs, validation_split=0.2)
        training_time = time.time() - start_time

        eval_metrics = lstm_model.evaluate(self.X_test, self.y_test)
        accuracy = eval_metrics['accuracy']

        # Ensure no division by zero or overly small numbers for training time
        fitness = (W_acc * accuracy) / (1 + W_time * training_time)
        print(f"Chromosome fitness: Accuracy={accuracy:.4f}, Training Time={training_time:.2f}s, Fitness={fitness:.4f}")
        return fitness, accuracy, training_time

    def _select_parents(self, population_with_fitness):
        """
        Selects parents for crossover based on their fitness (e.g., roulette wheel selection).
        """
        # [cite_start]Sort population by fitness in descending order [cite: 107]
        sorted_population = sorted(population_with_fitness, key=lambda x: x['fitness'], reverse=True)
        # Select the top chromosomes for mating pool (simplified for demonstration)
        mating_pool_size = int(len(sorted_population) * self.crossover_rate)
        parents = sorted_population[:mating_pool_size]
        print(f"Selected {len(parents)} parents for crossover.")
        return [p['chromosome'] for p in parents]

    def _crossover(self, parent1, parent2):
        """
        Performs crossover between two parent chromosomes.
        Simple single-point crossover for demonstration.
        """
        child1 = parent1.copy()
        child2 = parent2.copy()
        
        # Crossover point (e.g., swap num_hidden_units)
        child1['num_hidden_units'] = parent2['num_hidden_units']
        child2['num_hidden_units'] = parent1['num_hidden_units']
        print("Crossover performed.")
        return child1, child2

    def _mutate(self, chromosome):
        """
        Applies mutation to a chromosome.
        Randomly adjust hidden units or epochs.
        """
        if random.random() < self.mutation_rate:
            if random.random() < 0.5:
                chromosome['num_hidden_units'] = random.randint(10, 50)
                print(f"Mutated num_hidden_units to {chromosome['num_hidden_units']}")
            else:
                chromosome['max_epochs'] = random.randint(5, 15)
                print(f"Mutated max_epochs to {chromosome['max_epochs']}")
        return chromosome

    def optimize(self):
        """
        Runs the genetic algorithm optimization process.
        """
        population = self._initialize_population()
        best_chromosome = None
        best_fitness = -np.inf

        for generation in range(self.num_generations):
            print(f"\n--- Generation {generation + 1}/{self.num_generations} ---")
            population_with_fitness = []
            for i, chromosome in enumerate(population):
                print(f"Calculating fitness for chromosome {i+1}/{len(population)}: {chromosome}")
                fitness, accuracy, training_time = self._calculate_fitness(chromosome)
                population_with_fitness.append({'chromosome': chromosome, 'fitness': fitness, 
                                                'accuracy': accuracy, 'training_time': training_time})
                
                if fitness > best_fitness:
                    best_fitness = fitness
                    best_chromosome = chromosome
                    print(f"New best chromosome found: {best_chromosome}, Fitness: {best_fitness:.4f}")

            # Select parents for the next generation
            parents = self._select_parents(population_with_fitness)

            # Create next generation
            next_population = []
            # Keep the best from current population (elitism - simplified)
            next_population.append(best_chromosome) 
            
            while len(next_population) < self.pop_size:
                if len(parents) < 2: # Ensure enough parents for crossover
                    parent1 = random.choice(population_with_fitness)['chromosome']
                    parent2 = random.choice(population_with_fitness)['chromosome']
                else:
                    parent1, parent2 = random.sample(parents, 2)
                
                if random.random() < self.crossover_rate:
                    child1, child2 = self._crossover(parent1, parent2)
                    next_population.append(self._mutate(child1))
                    if len(next_population) < self.pop_size:
                        next_population.append(self._mutate(child2))
                else:
                    next_population.append(self._mutate(parent1))
                    if len(next_population) < self.pop_size:
                        next_population.append(self._mutate(parent2))

            population = next_population[:self.pop_size] # Ensure population size is maintained
            print(f"Population size for next generation: {len(population)}")

        print("\n--- GA Optimization Complete ---")
        print(f"Optimal LSTM parameters found: {best_chromosome}")
        return best_chromosome

# Main execution flow (conceptual)
if __name__ == "__main__":
    print("Starting the Ferroresonance Detection System (Conceptual)")

    # 1. Data Loading and Preprocessing (Simulated)
    # In a real scenario, this would involve loading voltage data and applying DWT.
    # For demonstration, DataLoader generates dummy DWT-like features.
    data_loader = DataLoader(num_samples=1000, sequence_length=18, num_classes=2)
    X, y = data_loader.get_data()

    # Example of using WaveletTransform (conceptually)
    # Assuming 'X' here represents the raw voltage signals before feature extraction for this example
    # In practice, X would already contain the DWT coefficients.
    print("\n--- Demonstrating Wavelet Transform (Conceptual) ---")
    if X.shape[2] == 1: # If X is (samples, sequence_length, 1), assume sequence_length is the signal length
        # Pick one sample signal for demonstration
        sample_signal = X[0, :, 0] 
        wt_processor = WaveletTransform(wavelet='db4', level=6)
        approx_coeffs, detail_coeffs = wt_processor.decompose(sample_signal)
        # In a real system, these detail_coeffs would be flattened/reshaped and used as input to LSTM.
        print(f"First detail coefficient set shape: {detail_coeffs[0].shape}")
    else:
        print("Skipping detailed DWT demo as dummy data is already in feature format.")


    # 2. Genetic Algorithm for LSTM Optimization
    print("\n--- Starting Genetic Algorithm Optimization ---")
    genetic_optimizer = GeneticAlgorithm(data_loader)
    optimal_params = genetic_optimizer.optimize()

    # 3. Final LSTM Model Training with Optimal Parameters
    print("\n--- Training Final LSTM Model with Optimal Parameters ---")
    final_lstm_model = LSTMModel(input_shape=(data_loader.sequence_length, 1),
                                 num_hidden_units=optimal_params['num_hidden_units'],
                                 learning_rate=0.001)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    final_lstm_model.train(X_train, y_train, epochs=optimal_params['max_epochs'])

    # 4. Final Evaluation
    print("\n--- Final Model Evaluation ---")
    final_metrics = final_lstm_model.evaluate(X_test, y_test)
    print("Final Model Performance:")
    for metric, value in final_metrics.items():
        print(f"  {metric.replace('_', ' ').title()}: {value:.4f}")

    print("\nFerroresonance Detection System (Conceptual) execution finished.")
